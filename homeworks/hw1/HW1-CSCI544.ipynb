{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import sklearn\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/waterdog/git/csci-544-applied-nlp/.venv/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/waterdog/git/csci-544-applied-nlp/.venv/lib/python3.11/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/waterdog/git/csci-544-applied-nlp/.venv/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/waterdog/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "# disable ssl verification\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('wordnet')\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/dxklp9_s1fg2bxbl4v970rgh0000gn/T/ipykernel_47012/2418398514.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"./amazon_reviews_us_Office_Products_v1_00.tsv\", sep='\\t', on_bad_lines='skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['marketplace', 'customer_id', 'review_id', 'product_id',\n",
      "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
      "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
      "       'review_headline', 'review_body', 'review_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Data Path: \"./amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
    "df = pd.read_csv(\"./amazon_reviews_us_Office_Products_v1_00.tsv\", sep='\\t', on_bad_lines='skip')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body star_rating\n",
      "0                                     Great product.           5\n",
      "1  What's to say about this commodity item except...           5\n",
      "2    Haven't used yet, but I am sure I will like it.           5\n",
      "3  Although this was labeled as &#34;new&#34; the...           1\n",
      "4                    Gorgeous colors and easy to use           4\n"
     ]
    }
   ],
   "source": [
    "df = df[['review_body', 'star_rating']]\n",
    "df = df.dropna()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star_rating\n",
      "5    1582704\n",
      "4     418348\n",
      "1     306967\n",
      "3     193680\n",
      "2     138381\n",
      "Name: count, dtype: int64\n",
      "star_rating\n",
      "positive    2001052\n",
      "negative     445348\n",
      "neutral      193680\n",
      "Name: count, dtype: int64\n",
      "Great product.\n",
      "Nice quality. Happy  with the item\n",
      "Although this was labeled as &#34;new&#34; the one I received clearly had been used. The box had previously been opened., and the shredder was dirty and the bin was partially full of shredded paper. What was worse is that the unit will not work properly. It is not possible to insert the paper bin so as to enable the shredder to run. It will not operate if the bin is not in place, but I could never get the unit to recognize that the paper bin was actually fully inserted. After cleaning everything thoroughly and vacuuming the paper bin area, it worked ONCE! After that I was unable to get it work at all. I returned the unit immediately for a refund. I feel Amazon misrepresented the  unit as &#34;new&#34; when clearly it was not.\n"
     ]
    }
   ],
   "source": [
    "# Filter out non-integer values\n",
    "df = df[pd.to_numeric(df['star_rating'], errors='coerce').notnull()]\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "\n",
    "def get_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 'positive'\n",
    "    elif rating == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "# Print the number of each star rating\n",
    "print(df['star_rating'].value_counts())\n",
    "    \n",
    "# Print the number of positive (>=4), neutral (=3), and negative reviews (<=2)\n",
    "print(df['star_rating'].apply(get_sentiment).value_counts())\n",
    "\n",
    "# Print a sample for each sentiment \n",
    "print(df[df['star_rating'] == 5]['review_body'].iloc[0])\n",
    "print(df[df['star_rating'] == 3]['review_body'].iloc[0])\n",
    "print(df[df['star_rating'] == 1]['review_body'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 100,000 reviews randomly from positive and negative classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  sentiment\n",
      "14057    Two out of four that I ordered came broken.  O...          0\n",
      "567073   I've purchased these a few times now (2 or 3 c...          1\n",
      "2605563  Let me start off by saying that I have owned b...          1\n",
      "2021081  dont like it cant understand it, dont know why...          0\n",
      "286020                                            I like:)          0\n"
     ]
    }
   ],
   "source": [
    "sample_size = 100000\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "positive = df[df['star_rating'] > 3].sample(sample_size)\n",
    "negative = df[df['star_rating'] < 3].sample(sample_size)\n",
    "\n",
    "df = pd.concat([positive, negative])\n",
    "\n",
    "def get_sentiment_label(rating):\n",
    "    if rating >= 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# replace star ratings with sentiment labels\n",
    "# 1 - positive, 0 - negative\n",
    "df['sentiment'] = df['star_rating'].apply(get_sentiment_label)\n",
    "\n",
    "# remove star_rating column\n",
    "df = df.drop(columns=['star_rating'])\n",
    "\n",
    "samples = [567073,  # contraction\n",
    "        2021257,\n",
    "        2021081,\n",
    "        ]\n",
    "\n",
    "# randomly peek 5 rows\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the reviews to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  sentiment\n",
      "2157864  when i first installed the 2 ink cartridges th...          0\n",
      "386445                                           wonderful          1\n",
      "2303164  after a short time, these chalk holders do not...          0\n",
      "2021257  my printer will not print black.  it was worki...          0\n",
      "2370679  there have been some negative reviews of this ...          1\n"
     ]
    }
   ],
   "source": [
    "# Convert the reviews to lowercase\n",
    "df['review_body'] = df['review_body'].str.lower()\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/dxklp9_s1fg2bxbl4v970rgh0000gn/T/ipykernel_47012/1388353943.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  df['review_body'] = df['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
      "/var/folders/cj/dxklp9_s1fg2bxbl4v970rgh0000gn/T/ipykernel_47012/1388353943.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  df['review_body'] = df['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  sentiment\n",
      "666065   my key doesnt fit, but it feel like its a nice...          0\n",
      "1535652  i bought these for my brother hl-2170w.  it us...          1\n",
      "1744505  desktop publishing supplies 11 mil greeting ca...          0\n",
      "2048041  i wanted red to accent my husbands office---it...          1\n",
      "2594805  i first purchased a cx5200 epson all-in-one.  ...          0\n"
     ]
    }
   ],
   "source": [
    "# Remove HTML tags\n",
    "df['review_body'] = df['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "\n",
    "# Remove URLs\n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.sub(r'http\\S+', '', x) or re.sub(r'www\\S+', '', x))\n",
    "\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  sentiment\n",
      "2027450  the ink was faulty i had to return but 2 stars...          0\n",
      "741690   these are not too expensive and seem to write ...          1\n",
      "1371358  my printer will not allow this cartridge to wo...          0\n",
      "1359687  the \"plain\" book they have at the funeral home...          1\n",
      "428969   truly a waste of time and money. purchased the...          0\n"
     ]
    }
   ],
   "source": [
    "def remove_spaces(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "df['review_body'] = df['review_body'].apply(remove_spaces)\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform contractions on the reviews, e.g., won’t →will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# perform contractions on the reviews, e.g., won’t → will not. include as many contractions in English that you can think of\n",
    "# Source: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "    \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is / how does\",\n",
    "    \"i'd\": \"i had / i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i shall / i will\",\n",
    "    \"i'll've\": \"i shall have / i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Remove multiple mappings of contractions\n",
    "for key, value in contractions.items():\n",
    "    contractions[key] = value.split('/')[0].strip()\n",
    "\n",
    "df = df.replace(contractions, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  sentiment\n",
      "2193476   like i am going to get cheated again   after ...          0\n",
      "1323775  the bulletin board met my expectations and ser...          1\n",
      "2583511  this is a poorly designed device and it does n...          0\n",
      "2137021  i like the camera itself  which comes with jus...          1\n",
      "267796   this ink works very good with my printer  just...          1\n"
     ]
    }
   ],
   "source": [
    "# remove non-alphabetical characters\n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/waterdog/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  sentiment\n",
      "471326   using years one kind durable notebooks cheap t...          1\n",
      "1396604  owned screen months could happier plenty mount...          1\n",
      "1964696  well designed fully satisfied speed quality pr...          1\n",
      "1221318                       item described fast delivery          1\n",
      "583530   stick dollar tree foam board paper well even h...          0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df['review_body'] = df['review_body'].apply(remove_stopwords)\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  sentiment\n",
      "1809821  flag curved rod till end keep curling top lett...          0\n",
      "1357716  black ink cartridge empty disappointed itwas r...          0\n",
      "2528099  idea well product work compatible digital isp ...          0\n",
      "2015331  disappointed find make two size standard diary...          0\n",
      "1318017  telephone handset arrived time advertised pric...          1\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df['review_body'] = df['review_body'].apply(lemmatize)\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-iDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# TF-iDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "X = vectorizer.fit_transform(df['review_body']).toarray()\n",
    "y = df['sentiment']\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.58      0.72     20087\n",
      "           1       0.69      0.97      0.81     19913\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.82      0.77      0.76     40000\n",
      "weighted avg       0.82      0.77      0.76     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Perceptron model on your training dataset using the sklearn built-in\n",
    "# implementation. Report Accuracy, Precision, Recall, and f1-score on both\n",
    "# the training and testing split of your dataset.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = Perceptron()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM model on your training dataset using the sklearn built-in\n",
    "# implementation. Report Accuracy, Precision, Recall, and f1-score on both\n",
    "# the training and testing split of your dataset.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model on your training dataset using the sklearn\n",
    "# built-in implementation. Report Accuracy, Precision, Recall, and f1-score on\n",
    "# both the training and testing split of your dataset.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
